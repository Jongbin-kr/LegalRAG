{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21371,"status":"ok","timestamp":1717988613890,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"-E12wS1WzZet","outputId":"4dc7724f-0504-42cf-dedc-ec6861fe2009"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/My Drive/univ/4/nlp/gpt\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/univ/4/nlp/gpt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19119,"status":"ok","timestamp":1717912471665,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"1uGu9K_52lH-","outputId":"dcd73035-1ca3-44cc-ca2f-69e496ff251d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting openai\n","  Downloading openai-1.33.0-py3-none-any.whl (325 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/325.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/325.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m303.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.3)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n","Installing collected packages: h11, httpcore, httpx, openai\n","Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.33.0\n"]}],"source":["!pip install openai"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7284202,"status":"ok","timestamp":1717919911556,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"lDxF2f5b2qIO","outputId":"f1877c61-e764-405c-a420-7c18d552860d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed 10 prompts. Elapsed time: 75.00 seconds.\n","Processed 20 prompts. Elapsed time: 158.15 seconds.\n","Processed 30 prompts. Elapsed time: 240.08 seconds.\n","Processed 40 prompts. Elapsed time: 325.25 seconds.\n","Processed 50 prompts. Elapsed time: 397.92 seconds.\n","Processed 60 prompts. Elapsed time: 478.65 seconds.\n","Processed 70 prompts. Elapsed time: 563.26 seconds.\n","Processed 80 prompts. Elapsed time: 652.61 seconds.\n","Processed 90 prompts. Elapsed time: 743.22 seconds.\n","Processed 100 prompts. Elapsed time: 835.06 seconds.\n","Processed 110 prompts. Elapsed time: 936.71 seconds.\n","Processed 120 prompts. Elapsed time: 1012.43 seconds.\n","Processed 130 prompts. Elapsed time: 1099.61 seconds.\n","Processed 140 prompts. Elapsed time: 1186.41 seconds.\n","Processed 150 prompts. Elapsed time: 1280.80 seconds.\n","Processed 160 prompts. Elapsed time: 1371.17 seconds.\n","Processed 170 prompts. Elapsed time: 1461.88 seconds.\n","Processed 180 prompts. Elapsed time: 1543.96 seconds.\n","Processed 190 prompts. Elapsed time: 1645.38 seconds.\n","Processed 200 prompts. Elapsed time: 1734.41 seconds.\n","Processed 210 prompts. Elapsed time: 1809.00 seconds.\n","Processed 220 prompts. Elapsed time: 1873.75 seconds.\n","Processed 230 prompts. Elapsed time: 1963.63 seconds.\n","Processed 240 prompts. Elapsed time: 2060.49 seconds.\n","Processed 250 prompts. Elapsed time: 2141.87 seconds.\n","Processed 260 prompts. Elapsed time: 2207.60 seconds.\n","Processed 270 prompts. Elapsed time: 2295.81 seconds.\n","Processed 280 prompts. Elapsed time: 2386.61 seconds.\n","Processed 290 prompts. Elapsed time: 2466.64 seconds.\n","Processed 300 prompts. Elapsed time: 2551.76 seconds.\n","Processed 310 prompts. Elapsed time: 2638.11 seconds.\n","Processed 320 prompts. Elapsed time: 2719.49 seconds.\n","Processed 330 prompts. Elapsed time: 2819.51 seconds.\n","Processed 340 prompts. Elapsed time: 2914.90 seconds.\n","Processed 350 prompts. Elapsed time: 3017.81 seconds.\n","Processed 360 prompts. Elapsed time: 3103.86 seconds.\n","Processed 370 prompts. Elapsed time: 3185.67 seconds.\n","Processed 380 prompts. Elapsed time: 3285.23 seconds.\n","Processed 390 prompts. Elapsed time: 3374.44 seconds.\n","Processed 400 prompts. Elapsed time: 3447.95 seconds.\n","Processed 410 prompts. Elapsed time: 3537.54 seconds.\n","Processed 420 prompts. Elapsed time: 3617.62 seconds.\n","Processed 430 prompts. Elapsed time: 3697.96 seconds.\n","Processed 440 prompts. Elapsed time: 3770.42 seconds.\n","Processed 450 prompts. Elapsed time: 3855.84 seconds.\n","Processed 460 prompts. Elapsed time: 3962.92 seconds.\n","Processed 470 prompts. Elapsed time: 4062.12 seconds.\n","Processed 480 prompts. Elapsed time: 4140.30 seconds.\n","Processed 490 prompts. Elapsed time: 4234.48 seconds.\n","Processed 500 prompts. Elapsed time: 4341.43 seconds.\n","Processed 510 prompts. Elapsed time: 4430.08 seconds.\n","Processed 520 prompts. Elapsed time: 4510.90 seconds.\n","Processed 530 prompts. Elapsed time: 4606.89 seconds.\n","Processed 540 prompts. Elapsed time: 4692.22 seconds.\n","Processed 550 prompts. Elapsed time: 4779.22 seconds.\n","Processed 560 prompts. Elapsed time: 4878.21 seconds.\n","Processed 570 prompts. Elapsed time: 4974.31 seconds.\n","Processed 580 prompts. Elapsed time: 5079.06 seconds.\n","Processed 590 prompts. Elapsed time: 5139.23 seconds.\n","Processed 600 prompts. Elapsed time: 5231.71 seconds.\n","Processed 610 prompts. Elapsed time: 5327.62 seconds.\n","Processed 620 prompts. Elapsed time: 5413.20 seconds.\n","Processed 630 prompts. Elapsed time: 5498.78 seconds.\n","Processed 640 prompts. Elapsed time: 5589.78 seconds.\n","Processed 650 prompts. Elapsed time: 5672.41 seconds.\n","Processed 660 prompts. Elapsed time: 5762.25 seconds.\n","Processed 670 prompts. Elapsed time: 5836.53 seconds.\n","Processed 680 prompts. Elapsed time: 5930.10 seconds.\n","Processed 690 prompts. Elapsed time: 5997.92 seconds.\n","Processed 700 prompts. Elapsed time: 6085.10 seconds.\n","Processed 710 prompts. Elapsed time: 6173.60 seconds.\n","Processed 720 prompts. Elapsed time: 6267.97 seconds.\n","Processed 730 prompts. Elapsed time: 6370.30 seconds.\n","Processed 740 prompts. Elapsed time: 6456.13 seconds.\n","Processed 750 prompts. Elapsed time: 6560.65 seconds.\n","Processed 760 prompts. Elapsed time: 6640.20 seconds.\n","Processed 770 prompts. Elapsed time: 6736.84 seconds.\n","Processed 780 prompts. Elapsed time: 6837.24 seconds.\n","Processed 790 prompts. Elapsed time: 6935.74 seconds.\n","Processed 800 prompts. Elapsed time: 7014.18 seconds.\n","Processed 810 prompts. Elapsed time: 7091.18 seconds.\n","Processed 820 prompts. Elapsed time: 7180.40 seconds.\n","Processed 830 prompts. Elapsed time: 7281.07 seconds.\n","Responses saved to baseline4_gpt_result.csv\n"]}],"source":["from openai import OpenAI\n","import pandas as pd\n","import time\n","\n","# OpenAI API 키 설정\n","api_key = 'YOUR_API'\n","client = OpenAI(api_key=api_key)\n","\n","def get_gpt3_response(prompt):\n","    response = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",  # GPT-3.5 모델 엔진 이름\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        max_tokens=4096,  # 원하는 응답 길이\n","        n=1,  # 하나의 응답 생성\n","        stop=None,\n","        temperature=0.7\n","    )\n","    return response.choices[0].message.content\n","\n","# CSV 파일에서 프롬프트 읽기\n","csv_file = 'test_data_with_retrieve_and_laws_normalized.csv'\n","df = pd.read_csv(csv_file)\n","\n","# 응답 저장을 위한 리스트 초기화\n","responses = []\n","start_time = time.time()\n","\n","# 각 프롬프트에 대해 GPT-3.5 응답 받기\n","for index, row in df.iterrows():\n","    prompt = '다음은 내가 처한 상황이야.'+row['자연어 쿼리']+'변호사로서 내 상황에 적용할 수 있는 법령과 그 이유를 설명해줘.'+'다음은 나와 유사한 상황 판례야.'+row['문서 범죄 사실']+'이 판례에서 적용된 법령과 그에 대한 설명이야'+row['문서 법설명']\n","    response = get_gpt3_response(prompt)\n","    responses.append(response)\n","    if (index + 1) % 10 == 0:\n","        elapsed_time = time.time() - start_time\n","        print(f'Processed {index + 1} prompts. Elapsed time: {elapsed_time:.2f} seconds.')\n","\n","# 응답을 새로운 열로 데이터프레임에 추가\n","df['response'] = responses\n","\n","# 결과를 새로운 CSV 파일로 저장\n","output_csv_file = 'baseline4_gpt_result.csv'\n","df.to_csv(output_csv_file, index=False)\n","\n","print(f'Responses saved to {output_csv_file}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2456,"status":"ok","timestamp":1717941711802,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"hNIYilZMYTCl","outputId":"a2d28b2a-be78-4083-fd0c-5d7575cf73d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 Score: 14.099046468977042%\n"]}],"source":["#baseline3-2 gpt f1\n","import pandas as pd\n","import numpy as np\n","from metrics import exact_match_score, accuracy, f1, qa_f1_score\n","\n","csv_file = 'baseline4_gpt_result.csv'\n","df = pd.read_csv(csv_file)\n","\n","# query와 response를 group으로 묶음\n","df['group'] = df.index // 5\n","\n","# 각 query마다 F1 점수가 가장 높은 response 선택\n","best_responses = []\n","\n","for group_id, group in df.groupby('group'):\n","    best_f1 = -1\n","    best_response = None\n","    ground_truth = group['쿼리 법령의 적용'].iloc[0]  # 같은 query에 대한 ground_truth는 동일하므로 첫번째 값을 사용\n","\n","    for index, row in group.iterrows():\n","        response = row['response']\n","        current_f1 = qa_f1_score(response, ground_truth)\n","\n","        if current_f1 > best_f1:\n","            best_f1 = current_f1\n","            best_response = response\n","\n","    best_responses.append((group_id, best_response, ground_truth))\n","\n","# best_responses에서 응답과 정답 추출\n","preds = [response for _, response, _ in best_responses]\n","ground_truths = [gt for _, _, gt in best_responses]\n","\n","\n","# F1 점수 계산\n","f1_score = f1(preds, [[gt] for gt in ground_truths])\n","print(f\"F1 Score: {f1_score}%\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10029,"status":"ok","timestamp":1717988746266,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"3Kt3rrKsZLEa","outputId":"8cc3835f-6350-4a7d-ef70-f0fa8325562b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=d3abe9044fb51ca1be5ee92956c17cb29307f74750d596c2083b368a107d8f89\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}],"source":["!pip install nltk rouge_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5764,"status":"ok","timestamp":1717941775384,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"nyFfiYlgZYoC","outputId":"74711202-af47-4c6d-839a-82e02a5809ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 Score: 14.099046468977042%\n","BLEU Score: 1.5362255987973208%\n","BLEU-1 Score: 7.458959326881392%\n","BLEU-2 Score: 3.614685043500401%\n","ROUGE-1 Score: 57.40216732040354%\n","ROUGE-2 Score: 30.942783696409844%\n","ROUGE-L Score: 49.89028182177195%\n"]}],"source":["#baseline3-2 gpt\n","import pandas as pd\n","import numpy as np\n","from metrics import exact_match_score, f1, qa_f1_score\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from rouge_score import rouge_scorer\n","\n","csv_file = 'baseline4_gpt_result.csv'\n","df = pd.read_csv(csv_file)\n","\n","# query와 response를 group으로 묶음\n","df['group'] = df.index // 5\n","\n","# 각 query마다 최고 점수의 response 선택\n","best_responses = []\n","\n","# BLEU 및 ROUGE 계산 준비\n","smoothing = SmoothingFunction().method4\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","# 각 메트릭 점수를 저장할 리스트\n","max_f1_scores = []\n","max_bleu_scores = []\n","max_bleu1_scores = []\n","max_bleu2_scores = []\n","max_rouge1_scores = []\n","max_rouge2_scores = []\n","max_rougeL_scores = []\n","\n","for group_id, group in df.groupby('group'):\n","    ground_truth = group['쿼리 법령의 적용'].iloc[0]  # 같은 query에 대한 ground_truth는 동일하므로 첫번째 값을 사용\n","\n","    max_f1 = -1\n","    max_bleu = -1\n","    max_bleu1 = -1\n","    max_bleu2 = -1\n","    max_rouge1 = -1\n","    max_rouge2 = -1\n","    max_rougeL = -1\n","\n","    for index, row in group.iterrows():\n","        response = row['response']\n","\n","        # F1 점수 계산\n","        current_f1 = qa_f1_score(response, ground_truth)\n","        if current_f1 > max_f1:\n","            max_f1 = current_f1\n","\n","        # BLEU 점수 계산\n","        current_bleu = sentence_bleu([ground_truth.split()], response.split(), smoothing_function=smoothing)\n","        if current_bleu > max_bleu:\n","            max_bleu = current_bleu\n","\n","        current_bleu1 = sentence_bleu([ground_truth.split()], response.split(), weights=(1, 0, 0, 0), smoothing_function=smoothing)\n","        if current_bleu1 > max_bleu1:\n","            max_bleu1 = current_bleu1\n","\n","        current_bleu2 = sentence_bleu([ground_truth.split()], response.split(), weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n","        if current_bleu2 > max_bleu2:\n","            max_bleu2 = current_bleu2\n","\n","        # ROUGE 점수 계산\n","        scores = scorer.score(ground_truth, response)\n","        if scores['rouge1'].fmeasure > max_rouge1:\n","            max_rouge1 = scores['rouge1'].fmeasure\n","        if scores['rouge2'].fmeasure > max_rouge2:\n","            max_rouge2 = scores['rouge2'].fmeasure\n","        if scores['rougeL'].fmeasure > max_rougeL:\n","            max_rougeL = scores['rougeL'].fmeasure\n","\n","    # 최고 점수를 리스트에 추가\n","    max_f1_scores.append(max_f1)\n","    max_bleu_scores.append(max_bleu)\n","    max_bleu1_scores.append(max_bleu1)\n","    max_bleu2_scores.append(max_bleu2)\n","    max_rouge1_scores.append(max_rouge1)\n","    max_rouge2_scores.append(max_rouge2)\n","    max_rougeL_scores.append(max_rougeL)\n","\n","# 각 메트릭의 평균 점수 계산\n","avg_f1_score = 100 * np.mean(max_f1_scores)\n","avg_bleu_score = 100 * np.mean(max_bleu_scores)\n","avg_bleu1_score = 100 * np.mean(max_bleu1_scores)\n","avg_bleu2_score = 100 * np.mean(max_bleu2_scores)\n","avg_rouge1_score = 100 * np.mean(max_rouge1_scores)\n","avg_rouge2_score = 100 * np.mean(max_rouge2_scores)\n","avg_rougeL_score = 100 * np.mean(max_rougeL_scores)\n","\n","# 결과 출력\n","print(f\"F1 Score: {avg_f1_score}%\")\n","print(f\"BLEU Score: {avg_bleu_score}%\")\n","print(f\"BLEU-1 Score: {avg_bleu1_score}%\")\n","print(f\"BLEU-2 Score: {avg_bleu2_score}%\")\n","print(f\"ROUGE-1 Score: {avg_rouge1_score}%\")\n","print(f\"ROUGE-2 Score: {avg_rouge2_score}%\")\n","print(f\"ROUGE-L Score: {avg_rougeL_score}%\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2710,"status":"ok","timestamp":1717988750339,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"IVGYrfbVnaP7","outputId":"13eb8263-0afa-4663-9284-1520d803a59c"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 Score: 14.823709301441582%\n","BLEU Score: 1.8888332645585852%\n","BLEU-1 Score: 8.2332875297838%\n","BLEU-2 Score: 4.169220685203734%\n","ROUGE-1 Score: 58.91230938046649%\n","ROUGE-2 Score: 34.98721678847663%\n","ROUGE-L Score: 51.61833072441523%\n","Number of queries processed: 62\n","Number of responses per query: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"]}],"source":["#baseline 3-2 gpt_inter\n","import pandas as pd\n","import numpy as np\n","from metrics import exact_match_score, f1, qa_f1_score\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from rouge_score import rouge_scorer\n","\n","# CSV 파일 읽기\n","alpha3_csv_file='baseline3_AlphaMist_result.csv'\n","alpha_csv_file = 'AlphaMist_total_result.csv'\n","alpha_df = pd.read_csv(alpha_csv_file)\n","alpha3_df=pd.read_csv(alpha3_csv_file)\n","\n","csv_file = 'baseline4_gpt_result.csv'\n","df = pd.read_csv(csv_file)\n","\n","# 유효한 인덱스 필터링\n","valid_indices = alpha_df[alpha_df['답변'].notna() & alpha_df['RAG  답변'].notna()&alpha3_df['response'].notna()].index\n","\n","# group 컬럼 추가\n","df['group'] = df.index // 5\n","\n","# BLEU 및 ROUGE 계산 준비\n","smoothing = SmoothingFunction().method4\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","# 각 메트릭 점수를 저장할 리스트\n","max_f1_scores = []\n","max_bleu_scores = []\n","max_bleu1_scores = []\n","max_bleu2_scores = []\n","max_rouge1_scores = []\n","max_rouge2_scores = []\n","max_rougeL_scores = []\n","\n","# 각 valid_indices에 대해 계산된 응답의 수를 저장할 리스트\n","num_responses = []\n","\n","for idx in valid_indices:\n","    alpha_query = alpha_df.loc[idx, '자연어 쿼리']\n","    group = df[df['group'] == idx]\n","\n","    # 쿼리가 같은지 확인\n","    df_query = group['자연어 쿼리'].unique()\n","    if len(df_query) == 1 and alpha_query == df_query[0]:\n","        ground_truth = group['쿼리 법령의 적용'].iloc[0]  # 같은 query에 대한 ground_truth는 동일하므로 첫번째 값을 사용\n","        max_f1 = -1\n","        max_bleu = -1\n","        max_bleu1 = -1\n","        max_bleu2 = -1\n","        max_rouge1 = -1\n","        max_rouge2 = -1\n","        max_rougeL = -1\n","\n","        response_count = 0  # 현재 쿼리에 대해 계산된 응답의 수\n","\n","        for index, row in group.iterrows():\n","            response = row['response']\n","            response_count += 1\n","\n","            # F1 점수 계산\n","            current_f1 = qa_f1_score(response, ground_truth)\n","            if current_f1 > max_f1:\n","                max_f1 = current_f1\n","\n","            # BLEU 점수 계산\n","            current_bleu = sentence_bleu([ground_truth.split()], response.split(), smoothing_function=smoothing)\n","            if current_bleu > max_bleu:\n","                max_bleu = current_bleu\n","\n","            current_bleu1 = sentence_bleu([ground_truth.split()], response.split(), weights=(1, 0, 0, 0), smoothing_function=smoothing)\n","            if current_bleu1 > max_bleu1:\n","                max_bleu1 = current_bleu1\n","\n","            current_bleu2 = sentence_bleu([ground_truth.split()], response.split(), weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n","            if current_bleu2 > max_bleu2:\n","                max_bleu2 = current_bleu2\n","\n","            # ROUGE 점수 계산\n","            scores = scorer.score(ground_truth, response)\n","            if scores['rouge1'].fmeasure > max_rouge1:\n","                max_rouge1 = scores['rouge1'].fmeasure\n","            if scores['rouge2'].fmeasure > max_rouge2:\n","                max_rouge2 = scores['rouge2'].fmeasure\n","            if scores['rougeL'].fmeasure > max_rougeL:\n","                max_rougeL = scores['rougeL'].fmeasure\n","\n","        # 최고 점수를 리스트에 추가\n","        max_f1_scores.append(max_f1)\n","        max_bleu_scores.append(max_bleu)\n","        max_bleu1_scores.append(max_bleu1)\n","        max_bleu2_scores.append(max_bleu2)\n","        max_rouge1_scores.append(max_rouge1)\n","        max_rouge2_scores.append(max_rouge2)\n","        max_rougeL_scores.append(max_rougeL)\n","\n","        # 응답 수를 리스트에 추가\n","        num_responses.append(response_count)\n","\n","# 각 메트릭의 평균 점수 계산\n","avg_f1_score = 100 * np.mean(max_f1_scores)\n","avg_bleu_score = 100 * np.mean(max_bleu_scores)\n","avg_bleu1_score = 100 * np.mean(max_bleu1_scores)\n","avg_bleu2_score = 100 * np.mean(max_bleu2_scores)\n","avg_rouge1_score = 100 * np.mean(max_rouge1_scores)\n","avg_rouge2_score = 100 * np.mean(max_rouge2_scores)\n","avg_rougeL_score = 100 * np.mean(max_rougeL_scores)\n","\n","# 결과 출력\n","print(f\"F1 Score: {avg_f1_score}%\")\n","print(f\"BLEU Score: {avg_bleu_score}%\")\n","print(f\"BLEU-1 Score: {avg_bleu1_score}%\")\n","print(f\"BLEU-2 Score: {avg_bleu2_score}%\")\n","print(f\"ROUGE-1 Score: {avg_rouge1_score}%\")\n","print(f\"ROUGE-2 Score: {avg_rouge2_score}%\")\n","print(f\"ROUGE-L Score: {avg_rougeL_score}%\")\n","print(f\"Number of queries processed: {len(num_responses)}\")\n","print(f\"Number of responses per query: {num_responses}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1267,"status":"ok","timestamp":1717988944946,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"MHeHD8mx3s3o","outputId":"18269623-1076-4176-a560-fe94b84c40aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 Score: 11.780117491419723%\n","BLEU Score: 0.8682937008294254%\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"name":"stdout","output_type":"stream","text":["BLEU-1 Score: 6.365362998842759%\n","BLEU-2 Score: 3.015752221637736%\n","ROUGE-1 Score: 46.60442745757164%\n","ROUGE-2 Score: 24.963948326833126%\n","ROUGE-L Score: 40.14224654122961%\n"]}],"source":["#baseline 3-1 gpt_inter\n","import pandas as pd\n","import numpy as np\n","from metrics import exact_match_score, accuracy, f1, qa_f1_score\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from rouge_score import rouge_scorer\n","\n","# CSV 파일 읽기\n","alpha3_csv_file='baseline3_AlphaMist_result.csv'\n","alpha_csv_file = 'AlphaMist_total_result.csv'\n","alpha_df = pd.read_csv(alpha_csv_file)\n","alpha3_df=pd.read_csv(alpha3_csv_file)\n","\n","csv_file = 'baseline4_gpt_result.csv'\n","df = pd.read_csv(csv_file)\n","\n","# 유효한 인덱스 필터링\n","valid_indices = alpha_df[alpha_df['답변'].notna() & alpha_df['RAG  답변'].notna()&alpha3_df['response'].notna()].index\n","\n","# group 컬럼 추가\n","df['group'] = df.index // 5\n","\n","# 각 그룹의 첫 번째 행만 선택\n","first_rows = df.groupby('group').first().reset_index()\n","\n","# 유효한 인덱스에 해당하는 행만 필터링\n","valid_first_rows = first_rows[first_rows['group'].isin(valid_indices)]\n","\n","# 응답과 정답 추출\n","preds = valid_first_rows['response'].tolist()\n","ground_truths = valid_first_rows['쿼리 법령의 적용'].tolist()\n","\n","# F1 점수 계산\n","f1_score = f1(preds, [[gt] for gt in ground_truths])\n","print(f\"F1 Score: {f1_score}%\")\n","\n","# BLEU 점수 계산\n","bleu_scores = [sentence_bleu([gt.split()], pred.split()) for pred, gt in zip(preds, ground_truths)]\n","bleu_score = 100 * np.mean(bleu_scores)\n","print(f\"BLEU Score: {bleu_score}%\")\n","smoothing = SmoothingFunction().method4\n","\n","bleu1_scores = [sentence_bleu([gt.split()], pred.split(), weights=(1, 0, 0, 0), smoothing_function=smoothing) for pred, gt in zip(preds, ground_truths)]\n","bleu2_scores = [sentence_bleu([gt.split()], pred.split(), weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing) for pred, gt in zip(preds, ground_truths)]\n","\n","bleu1_score = 100 * np.mean(bleu1_scores)\n","bleu2_score = 100 * np.mean(bleu2_scores)\n","\n","print(f\"BLEU-1 Score: {bleu1_score}%\")\n","print(f\"BLEU-2 Score: {bleu2_score}%\")\n","# ROUGE 점수 계산\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","rouge1_scores = []\n","rouge2_scores = []\n","rougeL_scores = []\n","\n","for pred, gt in zip(preds, ground_truths):\n","    scores = scorer.score(gt, pred)\n","    rouge1_scores.append(scores['rouge1'].fmeasure)\n","    rouge2_scores.append(scores['rouge2'].fmeasure)\n","    rougeL_scores.append(scores['rougeL'].fmeasure)\n","\n","rouge1_score = 100 * np.mean(rouge1_scores)\n","rouge2_score = 100 * np.mean(rouge2_scores)\n","rougeL_score = 100 * np.mean(rougeL_scores)\n","\n","print(f\"ROUGE-1 Score: {rouge1_score}%\")\n","print(f\"ROUGE-2 Score: {rouge2_score}%\")\n","print(f\"ROUGE-L Score: {rougeL_score}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1094,"status":"ok","timestamp":1717942255098,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"pgXY1ahL6It4","outputId":"f17a42f9-652a-412d-9b1a-3855eb23b0d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 Score: 10.49078617479166%\n","BLEU Score: 0.502025380754265%\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 2-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 3-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n","/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n","The hypothesis contains 0 counts of 4-gram overlaps.\n","Therefore the BLEU score evaluates to 0, independently of\n","how many N-gram overlaps of lower order it contains.\n","Consider using lower n-gram order or use SmoothingFunction()\n","  warnings.warn(_msg)\n"]},{"name":"stdout","output_type":"stream","text":["BLEU-1 Score: 5.346126828319042%\n","BLEU-2 Score: 2.406951055869683%\n","ROUGE-1 Score: 43.49382991505471%\n","ROUGE-2 Score: 20.37837296374267%\n","ROUGE-L Score: 37.37995275471785%\n"]}],"source":["#baseline3-1 gpt\n","import pandas as pd\n","import numpy as np\n","from metrics import exact_match_score, accuracy, f1, qa_f1_score\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from rouge_score import rouge_scorer\n","\n","# CSV 파일 읽기\n","csv_file = 'baseline4_gpt_result.csv'\n","df = pd.read_csv(csv_file)\n","\n","# query와 response를 group으로 묶음\n","df['group'] = df.index // 5\n","\n","first_rows = df.groupby('group').first().reset_index(drop=True)\n","\n","# first_rows에서 응답과 정답 추출\n","preds = first_rows['response'].tolist()\n","ground_truths = first_rows['쿼리 법령의 적용'].tolist()\n","\n","# F1 점수 계산\n","f1_score = f1(preds, [[gt] for gt in ground_truths])\n","print(f\"F1 Score: {f1_score}%\")\n","\n","bleu_scores = [sentence_bleu([gt.split()], pred.split()) for pred, gt in zip(preds, ground_truths)]\n","bleu_score = 100 * np.mean(bleu_scores)\n","print(f\"BLEU Score: {bleu_score}%\")\n","smoothing = SmoothingFunction().method4\n","\n","bleu1_scores = [sentence_bleu([gt.split()], pred.split(), weights=(1, 0, 0, 0), smoothing_function=smoothing) for pred, gt in zip(preds, ground_truths)]\n","bleu2_scores = [sentence_bleu([gt.split()], pred.split(), weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing) for pred, gt in zip(preds, ground_truths)]\n","\n","bleu1_score = 100 * np.mean(bleu1_scores)\n","bleu2_score = 100 * np.mean(bleu2_scores)\n","\n","print(f\"BLEU-1 Score: {bleu1_score}%\")\n","print(f\"BLEU-2 Score: {bleu2_score}%\")\n","\n","# ROUGE 점수 계산\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","rouge1_scores = []\n","rouge2_scores = []\n","rougeL_scores = []\n","\n","for pred, gt in zip(preds, ground_truths):\n","    scores = scorer.score(gt, pred)\n","    rouge1_scores.append(scores['rouge1'].fmeasure)\n","    rouge2_scores.append(scores['rouge2'].fmeasure)\n","    rougeL_scores.append(scores['rougeL'].fmeasure)\n","\n","rouge1_score = 100 * np.mean(rouge1_scores)\n","rouge2_score = 100 * np.mean(rouge2_scores)\n","rougeL_score = 100 * np.mean(rougeL_scores)\n","\n","print(f\"ROUGE-1 Score: {rouge1_score}%\")\n","print(f\"ROUGE-2 Score: {rouge2_score}%\")\n","print(f\"ROUGE-L Score: {rougeL_score}%\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOrxqxc3yJ/JRslycgedB6y","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
