{"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"elapsed":2316,"status":"error","timestamp":1717989288162,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"NDLh4uSwJ54f","outputId":"27cdb70b-31a6-4f26-fadd-4efdad7a1fdb"},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-4cb1fe8c6f30>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/My Drive/univ/4/nlp/gpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/univ/4/nlp/gpt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8812,"status":"ok","timestamp":1717161326253,"user":{"displayName":"Boyoung Kim","userId":"15564693225461891315"},"user_tz":-540},"id":"HCDlQdbSJ992","outputId":"15ed74f3-94dc-4fab-e010-91ad3f697293"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting openai\n","  Downloading openai-1.30.5-py3-none-any.whl (320 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/320.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n","Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n","Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n","Installing collected packages: h11, httpcore, httpx, openai\n","Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.30.5\n"]}],"source":["!pip install openai"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8782725,"status":"ok","timestamp":1717170341986,"user":{"displayName":"Boyoung Kim","userId":"15564693225461891315"},"user_tz":-540},"id":"78Yn-0CXJ_bh","outputId":"be2398da-04dc-4205-8f39-1b8391fb0ea4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Processed 10 prompts. Elapsed time: 102.14 seconds.\n","Processed 20 prompts. Elapsed time: 218.39 seconds.\n","Processed 30 prompts. Elapsed time: 315.04 seconds.\n","Processed 40 prompts. Elapsed time: 421.23 seconds.\n","Processed 50 prompts. Elapsed time: 515.05 seconds.\n","Processed 60 prompts. Elapsed time: 642.68 seconds.\n","Processed 70 prompts. Elapsed time: 739.62 seconds.\n","Processed 80 prompts. Elapsed time: 833.90 seconds.\n","Processed 90 prompts. Elapsed time: 943.74 seconds.\n","Processed 100 prompts. Elapsed time: 1057.97 seconds.\n","Processed 110 prompts. Elapsed time: 1156.32 seconds.\n","Processed 120 prompts. Elapsed time: 1252.38 seconds.\n","Processed 130 prompts. Elapsed time: 1375.10 seconds.\n","Processed 140 prompts. Elapsed time: 1482.89 seconds.\n","Processed 150 prompts. Elapsed time: 1589.34 seconds.\n","Processed 160 prompts. Elapsed time: 1700.39 seconds.\n","Processed 170 prompts. Elapsed time: 1816.28 seconds.\n","Processed 180 prompts. Elapsed time: 1906.15 seconds.\n","Processed 190 prompts. Elapsed time: 2020.93 seconds.\n","Processed 200 prompts. Elapsed time: 2131.34 seconds.\n","Processed 210 prompts. Elapsed time: 2236.91 seconds.\n","Processed 220 prompts. Elapsed time: 2332.23 seconds.\n","Processed 230 prompts. Elapsed time: 2454.44 seconds.\n","Processed 240 prompts. Elapsed time: 2567.56 seconds.\n","Processed 250 prompts. Elapsed time: 2674.38 seconds.\n","Processed 260 prompts. Elapsed time: 2776.83 seconds.\n","Processed 270 prompts. Elapsed time: 2887.40 seconds.\n","Processed 280 prompts. Elapsed time: 2974.98 seconds.\n","Processed 290 prompts. Elapsed time: 3089.36 seconds.\n","Processed 300 prompts. Elapsed time: 3209.20 seconds.\n","Processed 310 prompts. Elapsed time: 3323.69 seconds.\n","Processed 320 prompts. Elapsed time: 3435.64 seconds.\n","Processed 330 prompts. Elapsed time: 3557.44 seconds.\n","Processed 340 prompts. Elapsed time: 3674.80 seconds.\n","Processed 350 prompts. Elapsed time: 3790.32 seconds.\n","Processed 360 prompts. Elapsed time: 3889.58 seconds.\n","Processed 370 prompts. Elapsed time: 3997.56 seconds.\n","Processed 380 prompts. Elapsed time: 4122.25 seconds.\n","Processed 390 prompts. Elapsed time: 4223.73 seconds.\n","Processed 400 prompts. Elapsed time: 4321.22 seconds.\n","Processed 410 prompts. Elapsed time: 4432.23 seconds.\n","Processed 420 prompts. Elapsed time: 4527.50 seconds.\n","Processed 430 prompts. Elapsed time: 4623.14 seconds.\n","Processed 440 prompts. Elapsed time: 4711.64 seconds.\n","Processed 450 prompts. Elapsed time: 4809.67 seconds.\n","Processed 460 prompts. Elapsed time: 4930.21 seconds.\n","Processed 470 prompts. Elapsed time: 5038.90 seconds.\n","Processed 480 prompts. Elapsed time: 5131.04 seconds.\n","Processed 490 prompts. Elapsed time: 5260.15 seconds.\n","Processed 500 prompts. Elapsed time: 5360.82 seconds.\n","Processed 510 prompts. Elapsed time: 5478.21 seconds.\n","Processed 520 prompts. Elapsed time: 5572.27 seconds.\n","Processed 530 prompts. Elapsed time: 5688.96 seconds.\n","Processed 540 prompts. Elapsed time: 5818.33 seconds.\n","Processed 550 prompts. Elapsed time: 5923.59 seconds.\n","Processed 560 prompts. Elapsed time: 6041.73 seconds.\n","Processed 570 prompts. Elapsed time: 6152.19 seconds.\n","Processed 580 prompts. Elapsed time: 6262.07 seconds.\n","Processed 590 prompts. Elapsed time: 6352.26 seconds.\n","Processed 600 prompts. Elapsed time: 6458.34 seconds.\n","Processed 610 prompts. Elapsed time: 6545.36 seconds.\n","Processed 620 prompts. Elapsed time: 6689.98 seconds.\n","Processed 630 prompts. Elapsed time: 6789.93 seconds.\n","Processed 640 prompts. Elapsed time: 6906.52 seconds.\n","Processed 650 prompts. Elapsed time: 6992.41 seconds.\n","Processed 660 prompts. Elapsed time: 7094.81 seconds.\n","Processed 670 prompts. Elapsed time: 7204.49 seconds.\n","Processed 680 prompts. Elapsed time: 7296.72 seconds.\n","Processed 690 prompts. Elapsed time: 7387.24 seconds.\n","Processed 700 prompts. Elapsed time: 7490.90 seconds.\n","Processed 710 prompts. Elapsed time: 7583.81 seconds.\n","Processed 720 prompts. Elapsed time: 7663.09 seconds.\n","Processed 730 prompts. Elapsed time: 7767.89 seconds.\n","Processed 740 prompts. Elapsed time: 7871.96 seconds.\n","Processed 750 prompts. Elapsed time: 7992.85 seconds.\n","Processed 760 prompts. Elapsed time: 8090.52 seconds.\n","Processed 770 prompts. Elapsed time: 8181.83 seconds.\n","Processed 780 prompts. Elapsed time: 8292.19 seconds.\n","Processed 790 prompts. Elapsed time: 8394.19 seconds.\n","Processed 800 prompts. Elapsed time: 8485.58 seconds.\n","Processed 810 prompts. Elapsed time: 8578.51 seconds.\n","Processed 820 prompts. Elapsed time: 8682.59 seconds.\n","Processed 830 prompts. Elapsed time: 8781.42 seconds.\n","Responses saved to baseline2_gpt_result.csv\n"]}],"source":["from openai import OpenAI\n","import pandas as pd\n","import time\n","\n","# OpenAI API 키 설정\n","api_key = 'YOUR_API'\n","client = OpenAI(api_key=api_key)\n","\n","def get_gpt3_response(prompt):\n","    response = client.chat.completions.create(\n","        model=\"gpt-3.5-turbo\",  # GPT-3.5 모델 엔진 이름\n","        messages=[\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": prompt}\n","        ],\n","        max_tokens=4096,  # 원하는 응답 길이\n","        n=1,  # 하나의 응답 생성\n","        stop=None,\n","        temperature=0.7\n","    )\n","    return response.choices[0].message.content\n","\n","# CSV 파일에서 프롬프트 읽기\n","csv_file = 'test_data_with_retrieve_normalized.csv'\n","df = pd.read_csv(csv_file)\n","\n","# 응답 저장을 위한 리스트 초기화\n","responses = []\n","start_time = time.time()\n","\n","# 각 프롬프트에 대해 GPT-3.5 응답 받기\n","for index, row in df.iterrows():\n","    prompt = '다음은 내가 처한 상황이야.'+row['query']+'다음은 나와 유사한 상황 판례야.'+row['fact']+'변호사로서 내 상황에 적용할 수 있는 법령과 그 이유를 설명해줘.'\n","    response = get_gpt3_response(prompt)\n","    responses.append(response)\n","    if (index + 1) % 10 == 0:\n","        elapsed_time = time.time() - start_time\n","        print(f'Processed {index + 1} prompts. Elapsed time: {elapsed_time:.2f} seconds.')\n","\n","# 응답을 새로운 열로 데이터프레임에 추가\n","df['response'] = responses\n","\n","# 결과를 새로운 CSV 파일로 저장\n","output_csv_file = 'baseline2_gpt_result.csv'\n","df.to_csv(output_csv_file, index=False)\n","\n","print(f'Responses saved to {output_csv_file}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3871,"status":"ok","timestamp":1717905025996,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"hShWzRGEVhju","outputId":"fd15dc91-a8be-4a62-f8bd-9d4913d0e08f"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 Score: 5.174067875261229%\n","BLEU Score: 0.48563415346577243%\n","BLEU-1 Score: 3.5441472490583354%\n","BLEU-2 Score: 1.2658111757854589%\n","ROUGE-1 Score: 22.961889440718974%\n","ROUGE-2 Score: 8.110033393499155%\n","ROUGE-L Score: 21.095906176468997%\n"]}],"source":["#baseline2-2 gpt\n","import pandas as pd\n","import numpy as np\n","from metrics import exact_match_score, f1, qa_f1_score\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from rouge_score import rouge_scorer\n","\n","csv_file = 'baseline2_gpt_result.csv'\n","df = pd.read_csv(csv_file)\n","\n","# query와 response를 group으로 묶음\n","df['group'] = df.index // 5\n","\n","# 각 query마다 최고 점수의 response 선택\n","best_responses = []\n","\n","# BLEU 및 ROUGE 계산 준비\n","smoothing = SmoothingFunction().method4\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","# 각 메트릭 점수를 저장할 리스트\n","max_f1_scores = []\n","max_bleu_scores = []\n","max_bleu1_scores = []\n","max_bleu2_scores = []\n","max_rouge1_scores = []\n","max_rouge2_scores = []\n","max_rougeL_scores = []\n","\n","for group_id, group in df.groupby('group'):\n","    ground_truth = group['laws'].iloc[0]  # 같은 query에 대한 ground_truth는 동일하므로 첫번째 값을 사용\n","\n","    max_f1 = -1\n","    max_bleu = -1\n","    max_bleu1 = -1\n","    max_bleu2 = -1\n","    max_rouge1 = -1\n","    max_rouge2 = -1\n","    max_rougeL = -1\n","\n","    for index, row in group.iterrows():\n","        response = row['response']\n","\n","        # F1 점수 계산\n","        current_f1 = qa_f1_score(response, ground_truth)\n","        if current_f1 > max_f1:\n","            max_f1 = current_f1\n","\n","        # BLEU 점수 계산\n","        current_bleu = sentence_bleu([ground_truth.split()], response.split(), smoothing_function=smoothing)\n","        if current_bleu > max_bleu:\n","            max_bleu = current_bleu\n","\n","        current_bleu1 = sentence_bleu([ground_truth.split()], response.split(), weights=(1, 0, 0, 0), smoothing_function=smoothing)\n","        if current_bleu1 > max_bleu1:\n","            max_bleu1 = current_bleu1\n","\n","        current_bleu2 = sentence_bleu([ground_truth.split()], response.split(), weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n","        if current_bleu2 > max_bleu2:\n","            max_bleu2 = current_bleu2\n","\n","        # ROUGE 점수 계산\n","        scores = scorer.score(ground_truth, response)\n","        if scores['rouge1'].fmeasure > max_rouge1:\n","            max_rouge1 = scores['rouge1'].fmeasure\n","        if scores['rouge2'].fmeasure > max_rouge2:\n","            max_rouge2 = scores['rouge2'].fmeasure\n","        if scores['rougeL'].fmeasure > max_rougeL:\n","            max_rougeL = scores['rougeL'].fmeasure\n","\n","    # 최고 점수를 리스트에 추가\n","    max_f1_scores.append(max_f1)\n","    max_bleu_scores.append(max_bleu)\n","    max_bleu1_scores.append(max_bleu1)\n","    max_bleu2_scores.append(max_bleu2)\n","    max_rouge1_scores.append(max_rouge1)\n","    max_rouge2_scores.append(max_rouge2)\n","    max_rougeL_scores.append(max_rougeL)\n","\n","# 각 메트릭의 평균 점수 계산\n","avg_f1_score = 100 * np.mean(max_f1_scores)\n","avg_bleu_score = 100 * np.mean(max_bleu_scores)\n","avg_bleu1_score = 100 * np.mean(max_bleu1_scores)\n","avg_bleu2_score = 100 * np.mean(max_bleu2_scores)\n","avg_rouge1_score = 100 * np.mean(max_rouge1_scores)\n","avg_rouge2_score = 100 * np.mean(max_rouge2_scores)\n","avg_rougeL_score = 100 * np.mean(max_rougeL_scores)\n","\n","# 결과 출력\n","print(f\"F1 Score: {avg_f1_score}%\")\n","print(f\"BLEU Score: {avg_bleu_score}%\")\n","print(f\"BLEU-1 Score: {avg_bleu1_score}%\")\n","print(f\"BLEU-2 Score: {avg_bleu2_score}%\")\n","print(f\"ROUGE-1 Score: {avg_rouge1_score}%\")\n","print(f\"ROUGE-2 Score: {avg_rouge2_score}%\")\n","print(f\"ROUGE-L Score: {avg_rougeL_score}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7694,"status":"ok","timestamp":1717989149465,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"9nFg8ibq7K4E","outputId":"1a0cb3e4-096b-485d-8c1b-042e96b67e86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=85260b81b77e4ef5ee4cc78d697b85e3d43771043c0290106b5b7af6b40ec213\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}],"source":["!pip install nltk rouge_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6879,"status":"ok","timestamp":1717989164753,"user":{"displayName":"김보영","userId":"03119726252609230646"},"user_tz":-540},"id":"1rTIifYZIkVT","outputId":"437a67f1-f82c-4461-919e-91fcea60bd51"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 Score: 5.618963289658072%\n","BLEU Score: 0.4758430198439393%\n","BLEU-1 Score: 3.8015642327853545%\n","BLEU-2 Score: 1.3356593018791605%\n","ROUGE-1 Score: 26.37654900344042%\n","ROUGE-2 Score: 9.636342962024552%\n","ROUGE-L Score: 23.574246870838%\n","Number of queries processed: 62\n","Number of responses per query: [5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"]}],"source":["#baseline2-2 gpt_inter\n","import pandas as pd\n","import numpy as np\n","from metrics import exact_match_score, f1, qa_f1_score\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from rouge_score import rouge_scorer\n","\n","# CSV 파일 읽기\n","alpha_csv_file = 'AlphaMist_total_result.csv'\n","alpha_df = pd.read_csv(alpha_csv_file)\n","csv_file = 'baseline2_gpt_result.csv'\n","df = pd.read_csv(csv_file)\n","alpha3_csv_file='baseline3_AlphaMist_result.csv'\n","alpha3_df=pd.read_csv(alpha3_csv_file)\n","\n","\n","# 유효한 인덱스 필터링\n","valid_indices = alpha_df[alpha_df['답변'].notna() & alpha_df['RAG  답변'].notna()&alpha3_df['response'].notna()].index\n","# group 컬럼 추가\n","df['group'] = df.index // 5\n","\n","# BLEU 및 ROUGE 계산 준비\n","smoothing = SmoothingFunction().method4\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","# 각 메트릭 점수를 저장할 리스트\n","max_f1_scores = []\n","max_bleu_scores = []\n","max_bleu1_scores = []\n","max_bleu2_scores = []\n","max_rouge1_scores = []\n","max_rouge2_scores = []\n","max_rougeL_scores = []\n","\n","# 각 valid_indices에 대해 계산된 응답의 수를 저장할 리스트\n","num_responses = []\n","\n","for idx in valid_indices:\n","    alpha_query = alpha_df.loc[idx, '자연어 쿼리']\n","    group = df[df['group'] == idx]\n","\n","    # 쿼리가 같은지 확인\n","    df_query = group['query'].unique()\n","    if len(df_query) == 1 and alpha_query == df_query[0]:\n","        ground_truth = group['laws'].iloc[0]  # 같은 query에 대한 ground_truth는 동일하므로 첫번째 값을 사용\n","        max_f1 = -1\n","        max_bleu = -1\n","        max_bleu1 = -1\n","        max_bleu2 = -1\n","        max_rouge1 = -1\n","        max_rouge2 = -1\n","        max_rougeL = -1\n","\n","        response_count = 0  # 현재 쿼리에 대해 계산된 응답의 수\n","\n","        for index, row in group.iterrows():\n","            response = row['response']\n","            response_count += 1\n","\n","            # F1 점수 계산\n","            current_f1 = qa_f1_score(response, ground_truth)\n","            if current_f1 > max_f1:\n","                max_f1 = current_f1\n","\n","            # BLEU 점수 계산\n","            current_bleu = sentence_bleu([ground_truth.split()], response.split(), smoothing_function=smoothing)\n","            if current_bleu > max_bleu:\n","                max_bleu = current_bleu\n","\n","            current_bleu1 = sentence_bleu([ground_truth.split()], response.split(), weights=(1, 0, 0, 0), smoothing_function=smoothing)\n","            if current_bleu1 > max_bleu1:\n","                max_bleu1 = current_bleu1\n","\n","            current_bleu2 = sentence_bleu([ground_truth.split()], response.split(), weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n","            if current_bleu2 > max_bleu2:\n","                max_bleu2 = current_bleu2\n","\n","            # ROUGE 점수 계산\n","            scores = scorer.score(ground_truth, response)\n","            if scores['rouge1'].fmeasure > max_rouge1:\n","                max_rouge1 = scores['rouge1'].fmeasure\n","            if scores['rouge2'].fmeasure > max_rouge2:\n","                max_rouge2 = scores['rouge2'].fmeasure\n","            if scores['rougeL'].fmeasure > max_rougeL:\n","                max_rougeL = scores['rougeL'].fmeasure\n","\n","        # 최고 점수를 리스트에 추가\n","        max_f1_scores.append(max_f1)\n","        max_bleu_scores.append(max_bleu)\n","        max_bleu1_scores.append(max_bleu1)\n","        max_bleu2_scores.append(max_bleu2)\n","        max_rouge1_scores.append(max_rouge1)\n","        max_rouge2_scores.append(max_rouge2)\n","        max_rougeL_scores.append(max_rougeL)\n","\n","        # 응답 수를 리스트에 추가\n","        num_responses.append(response_count)\n","\n","# 각 메트릭의 평균 점수 계산\n","avg_f1_score = 100 * np.mean(max_f1_scores)\n","avg_bleu_score = 100 * np.mean(max_bleu_scores)\n","avg_bleu1_score = 100 * np.mean(max_bleu1_scores)\n","avg_bleu2_score = 100 * np.mean(max_bleu2_scores)\n","avg_rouge1_score = 100 * np.mean(max_rouge1_scores)\n","avg_rouge2_score = 100 * np.mean(max_rouge2_scores)\n","avg_rougeL_score = 100 * np.mean(max_rougeL_scores)\n","\n","# 결과 출력\n","print(f\"F1 Score: {avg_f1_score}%\")\n","print(f\"BLEU Score: {avg_bleu_score}%\")\n","print(f\"BLEU-1 Score: {avg_bleu1_score}%\")\n","print(f\"BLEU-2 Score: {avg_bleu2_score}%\")\n","print(f\"ROUGE-1 Score: {avg_rouge1_score}%\")\n","print(f\"ROUGE-2 Score: {avg_rouge2_score}%\")\n","print(f\"ROUGE-L Score: {avg_rougeL_score}%\")\n","print(f\"Number of queries processed: {len(num_responses)}\")\n","print(f\"Number of responses per query: {num_responses}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
